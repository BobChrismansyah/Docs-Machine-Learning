{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Docs Machine Learning \u00b6 Dokumentasi ini dibuat dari hasil Googling, Github AI, dan buku pengenalan Machine Learning. Kami TIDAK bertanggung-jawab atas KEAKURATAN mau pun KERUGIAN yang mungkin terjadi akibat memanfaatkan situs ini. HARAP MENANGGUNG SENDIRI SEGALA RISIKO! Jangan lupa mengungkapkan/ menuliskan penghargaan (acknowledgement) jika anda menggunakan bahan karya dari pihak lain. Situs ini berbasis \u201cGoogle Sana, Google Sini, Coba Itu, Coba Ini, Lalu Tanya-tanyi\u201d (GSGSCICILTT). Entah ini PLAGIAT, entah ini RISET, yang jelas tidak pernah ada klaim bahwa ini merupakan karya asli, serta belum tentu pula merupakan solusi terbaik, serta bukan untuk konsumsi Scopus :). Mohon kiranya memberikan tanggapan, terutama jika memiliki solusi alternatif. Semoga catatan ini akan bermanfaat di masa mendatang, saat sudah lupa cara menyelesaikan masalah trivia ini. Rahmat M. Samik-Ibrahim Kontribusi \u00b6 Jika menemukan kesalahan dalam penulisan kata atau ingin menambahkan konten baru, silahkan buat perubahan. Anda juga bisa menggunakan tombol dibagian kanan atas di setiap artikel/konten untuk menambahkan ke pull request. Jika tidak tau apa maksudnya, anda juga bisa membuat issue atau kirim email. Terima Kasih \u00b6 Contributor:","title":"Docs"},{"location":"#docs-machine-learning","text":"Dokumentasi ini dibuat dari hasil Googling, Github AI, dan buku pengenalan Machine Learning. Kami TIDAK bertanggung-jawab atas KEAKURATAN mau pun KERUGIAN yang mungkin terjadi akibat memanfaatkan situs ini. HARAP MENANGGUNG SENDIRI SEGALA RISIKO! Jangan lupa mengungkapkan/ menuliskan penghargaan (acknowledgement) jika anda menggunakan bahan karya dari pihak lain. Situs ini berbasis \u201cGoogle Sana, Google Sini, Coba Itu, Coba Ini, Lalu Tanya-tanyi\u201d (GSGSCICILTT). Entah ini PLAGIAT, entah ini RISET, yang jelas tidak pernah ada klaim bahwa ini merupakan karya asli, serta belum tentu pula merupakan solusi terbaik, serta bukan untuk konsumsi Scopus :). Mohon kiranya memberikan tanggapan, terutama jika memiliki solusi alternatif. Semoga catatan ini akan bermanfaat di masa mendatang, saat sudah lupa cara menyelesaikan masalah trivia ini. Rahmat M. Samik-Ibrahim","title":"Docs Machine Learning"},{"location":"#kontribusi","text":"Jika menemukan kesalahan dalam penulisan kata atau ingin menambahkan konten baru, silahkan buat perubahan. Anda juga bisa menggunakan tombol dibagian kanan atas di setiap artikel/konten untuk menambahkan ke pull request. Jika tidak tau apa maksudnya, anda juga bisa membuat issue atau kirim email.","title":"Kontribusi"},{"location":"#terima-kasih","text":"Contributor:","title":"Terima Kasih"},{"location":"about/","text":"graph LR A[Machine Learning] --> B[Supervised Learning] A --> C[Unsupervised Learning] A --> D[Reinforcement Learning] A --> E[Semi-Supervised Learning] B --> F[Regression] B --> G[Classification] B --> H[Deep Learning] F --> I[Linear] F --> J[Polynomial] G --> K[Logistic Regression] G --> L[Decision Tree] G --> M[SVM] G --> N[ANN] H --> O[CNN] H --> P[<a href='#'>RNN</a>] C --> Q[Clustering] Q --> R[K-Means] C --> U[Dimensionality Reduction] U --> T[PCA] C --> S[Anomaly Detection] E --> V[Self Training] E --> W[Low-density Separation] D --> X[Dynaics Programming] D --> Y[Monte Carlo Methods]","title":"About"},{"location":"docs/","text":"","title":"Index"},{"location":"docs/churn-prediction/","text":"Slide is underconstruction","title":"Churn Prediction"},{"location":"docs/clustering/","text":"Clustering \u00b6 Pengertian \u00b6 Clustering dalam machine learning adalah teknik pembelajaran mesin yang membagi set data menjadi beberapa kelompok (cluster) berdasarkan kesamaan fitur atau atribut. Tujuannya adalah untuk menemukan struktur dan pola dalam data dengan membagi item data menjadi kelompok-kelompok (cluster) yang memiliki kesamaan atribut. Penerapan clustering bermanfaat dalanm berbagai aplikasi seperti analisis pelanggan, sistem rekomedasi dan deteksi anomali. Contohnya, jika kita memiliki data latih tentang pelanggan yang membeli produk-produk tertentu, clustering dapat digunakan untuk membagi pelanggan ke dalam kelompok berdasarkan preferensi belanja mereka. Setiap kelompok yang terbentuk mewakili subpopulasi pelanggan yang memiliki preferensi belanja yang serupa. Ada beberapa algoritma clustering yang populer, seperti: K-Means Clustering: K-Means adalah algoritma clustering yang paling populer. Algoritma ini membagi data menjadi k kelompok, dimana k adalah jumlah cluster yang diinginkan. Setiap cluster memiliki pusat yang disebut centroid. Data yang berdekatan dengan centroid akan dimasukkan ke dalam cluster yang sama. Algoritma ini akan mengulang iterasi sampai centroid tidak berubah lagi. Hierarchical Clustering: Menggabungkan atau memisahkan cluster sampai tersisa satu cluster yang mencakup semua item data. Hierarchical clustering memiliki dua jenis, yaitu agglomerative dan divisive. DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Membentuk cluster bedasarkan densitas data. Data yang memiliki jumlah tetangga yang besar akan diklasifikasikan sebagai bagian cluster, sementara data yang jarang akan diidentifikasi sebagai noise. Setiap algoritma clustering memiliki kelebihan dan kekurangan masing-masing, dan pemilihan algoritma tergantung pada karakteristik data dan tujuan pemodelan. Namun, pada dasarnya, semua algoritma clustering menggunakan suatu metrik untuk menentukan kesamaan antar item data dan menggunakan metrik tersebut untuk membagi item data ke dalam cluster. Analoginya adalah membagi buah-buahan ke dalam beberapa keranjang berdasarkan jenisnya. Misalnya, kita memiliki keranjang buah-buahan yang berisi apel, pisang, dan anggur. Dalam hal ini, masing-masing jenis buah adalah sebuah cluster dan keranjang adalah representasi dari cluster. Proses clustering serupa dengan membagi buah-buahan ke dalam keranjang berdasarkan jenis buah, yang memiliki kesamaan atribut (seperti rasa, warna, dan tekstur). Analogi ini menunjukkan bahwa clustering adalah proses pembagian item-item data ke dalam kelompok-kelompok yang memiliki kesamaan atribut. Proses ini dapat digunakan untuk memahami dan mengeksplorasi struktur internal dalam data. Pengelompokan Data \u00b6","title":"Clustering"},{"location":"docs/clustering/#clustering","text":"","title":"Clustering"},{"location":"docs/clustering/#pengertian","text":"Clustering dalam machine learning adalah teknik pembelajaran mesin yang membagi set data menjadi beberapa kelompok (cluster) berdasarkan kesamaan fitur atau atribut. Tujuannya adalah untuk menemukan struktur dan pola dalam data dengan membagi item data menjadi kelompok-kelompok (cluster) yang memiliki kesamaan atribut. Penerapan clustering bermanfaat dalanm berbagai aplikasi seperti analisis pelanggan, sistem rekomedasi dan deteksi anomali. Contohnya, jika kita memiliki data latih tentang pelanggan yang membeli produk-produk tertentu, clustering dapat digunakan untuk membagi pelanggan ke dalam kelompok berdasarkan preferensi belanja mereka. Setiap kelompok yang terbentuk mewakili subpopulasi pelanggan yang memiliki preferensi belanja yang serupa. Ada beberapa algoritma clustering yang populer, seperti: K-Means Clustering: K-Means adalah algoritma clustering yang paling populer. Algoritma ini membagi data menjadi k kelompok, dimana k adalah jumlah cluster yang diinginkan. Setiap cluster memiliki pusat yang disebut centroid. Data yang berdekatan dengan centroid akan dimasukkan ke dalam cluster yang sama. Algoritma ini akan mengulang iterasi sampai centroid tidak berubah lagi. Hierarchical Clustering: Menggabungkan atau memisahkan cluster sampai tersisa satu cluster yang mencakup semua item data. Hierarchical clustering memiliki dua jenis, yaitu agglomerative dan divisive. DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Membentuk cluster bedasarkan densitas data. Data yang memiliki jumlah tetangga yang besar akan diklasifikasikan sebagai bagian cluster, sementara data yang jarang akan diidentifikasi sebagai noise. Setiap algoritma clustering memiliki kelebihan dan kekurangan masing-masing, dan pemilihan algoritma tergantung pada karakteristik data dan tujuan pemodelan. Namun, pada dasarnya, semua algoritma clustering menggunakan suatu metrik untuk menentukan kesamaan antar item data dan menggunakan metrik tersebut untuk membagi item data ke dalam cluster. Analoginya adalah membagi buah-buahan ke dalam beberapa keranjang berdasarkan jenisnya. Misalnya, kita memiliki keranjang buah-buahan yang berisi apel, pisang, dan anggur. Dalam hal ini, masing-masing jenis buah adalah sebuah cluster dan keranjang adalah representasi dari cluster. Proses clustering serupa dengan membagi buah-buahan ke dalam keranjang berdasarkan jenis buah, yang memiliki kesamaan atribut (seperti rasa, warna, dan tekstur). Analogi ini menunjukkan bahwa clustering adalah proses pembagian item-item data ke dalam kelompok-kelompok yang memiliki kesamaan atribut. Proses ini dapat digunakan untuk memahami dan mengeksplorasi struktur internal dalam data.","title":"Pengertian"},{"location":"docs/clustering/#pengelompokan-data","text":"","title":"Pengelompokan Data"},{"location":"docs/decision-tree/","text":"Slide is underconstruction","title":"Decision Tree"},{"location":"docs/logistic-regression/","text":"Slide is underconstruction","title":"Logistic Regression"},{"location":"docs/naive-bayes/","text":"Naive Bayes \u00b6 Pengertian \u00b6 Naive Bayes adalah salah satu algoritma klasifikasi yang paling sederhana dan paling cepat. Algoritma ini berdasarkan pada teorema Bayes dengan asumsi independensi antar variabel. Algoritma ini sangat cepat karena hanya melakukan perhitungan pada data yang ada saja, tidak memerlukan proses training. Algoritma ini sangat cocok untuk digunakan pada data yang memiliki banyak atribut, karena algoritma ini tidak memerlukan proses training. Keunggulan NB adalah efektif dan cepat sehingga NB bisa digunakan pada aplikasi spam filtering dan deteksi anomali di jaringan komputer. Cara Kerja NB \u00b6 NB adalah sekumpulan algoritma klasifikasi yuang dibangun berdasarkan Teori Bayes (Thomas Bayes, seorang ahli matematika dari Inggris di abad ke-18). Prinsipnya adalah menghitung seberapa tinggi kemubgkinan satu example dalam suatu observasi untuk masuk ke dalam suatu kelas, dengan memanfaatkan training dataset untuk menghitung kemungkinan setiap kelas berdasarkan nilai-nilai feature di dalamnya. Dalam Statistik teori ini dipakai untuk menjelaskan conditional probability , yaitu kemungkinan munculnya suatu kejadian A bila suatu kejadian B muncul, karena kejadian A bergantung pada kejadian B maka disebut conditional atau bersyarat. Sebagai contoh membuat model prediksi SMS termasuk pesan sampah atau bukan. Dalam kasus ini, feature yang digunakan adalah kata-kata yang ada di dalam SMS. Dengan menggunakan teori Bayes, kita bisa menghitung kemungkinan SMS termasuk pesan sampah atau bukan dengan menghitung kemungkinan setiap kata-kata yang ada di dalam SMS termasuk pesan sampah atau bukan. Pesan sampah kebanyakan berisi kata \"kredit\" katakanlah jumlahnya 10% dari seluruh pesan SMS, lalu ditulis sebagai P(\"kredit\") = 0,1. Lalu dari pengamatan sebelumnya diperoleh misalnya 30% dari seluruh pesan adalah pesan sampah, sehinggga ditulis P(sampah) = 0,3. Apabila keduanya digabungkan maka conditional propability dapat dihitung nilainya, yang menunjukkan kemungkinan SMS dianggap sampah jika mengandung kata \"kredit\". Secara matematis dapat ditulis sebagai berikut: \\(P(sampah|\"kredit\") = \\frac{P(\"kredit\"|sampah)P(sampah)}{P(\"kredit\")}\\) Apabila posterior propability melampaui suatu ambang batas , maka SMS dianggap sampah. Misalnya ambang batasnya adalah 0,5, maka SMS dianggap sampah apabila \\(P(sampah|\"kredit\") > 0,5\\) . Membuat Model NB \u00b6 Langkah pertama dalam membuat model NB adalah mengubah data yang ada menjadi suatu tabel frekuensi Berisi Kata Kredit Pesan SMS Ada Tidak Ada Jumlah Sampah 8 22 30 Bukan Sampah 2 68 30 Jumlah 10 90 100 Selanjutnya membuat tabel kemungkinan berdasarkan tabel frekuensi untuk mengukur likehood atau kemungkinan kemunculan kata \"Kredit\" Berisi Kata Kredit Pesan SMS Ada Tidak Ada Jumlah Kemungkinan Sampah 8/30=0,27 22/30=0,73 30 0,3 Bukan Sampah 2/70=0,03 68/70=0,97 70 0.7 Kemungkinan 10/100=0,1 90/100=0,9 P(\"Kredit\"|Sampah) = 0,27 P(Sampah) = 0,3 Kemungkinan kata \"kredit\" muncul di sluruh pesan SMS yaitu P(\"kredit\") = 0,1 Dengan menggunakan posterior probability maka dapat dihitung nilai kemungkinan SMS dianggap sampah jika mengandung kata \"kredit\" yaitu P(Sampah|\"kredit\") = 0,27 * 0,3 / 0,1 = 0,81 Nilainya posterior probability yang tinggi sehingga bisa menentukan pesan apapun yang berisi kata \"kredit\" adalah pasti pesan sampah. Rumus Conditional Probability \u00b6 Secara matematis: \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)} = \\frac{P(B \\cap A)}{P(B)}\\) Dengan: P(A) : kemungkinan A terjadi P(A|B) : kemungkinan A terjadi jika B terjadi P(B) : kemungkinan B terjadi P(B|A) : kemungkinan B terjadi jika A terjadi P(B \u2229 A) : kemungkinan B dan A terjadi Scikit-learn menyediakan pilihan untuk membuat model NB dengan menggunakan metode Gaussian, Multinomial dan Bernoulli. Metode Gaussian digunakan untuk data yang berupa angka, sedangkan metode Multinomial digunakan untuk data yang berupa kata-kata dan metode Bernoulli digunakan untuk data yang berupa kata-kata yang bernilai 0 atau 1. kelebihan dan kekurangan \u00b6 Kelebihan NB adalah mudah untuk diimplementasikan dan memiliki performa yang baik. Namun, NB memiliki kekurangan yaitu tidak dapat mengklasifikasikan data yang tidak terdapat dalam data latih. Selain itu, NB juga tidak dapat mengklasifikasikan data yang memiliki nilai yang sama.","title":"Naive Bayes"},{"location":"docs/naive-bayes/#naive-bayes","text":"","title":"Naive Bayes"},{"location":"docs/naive-bayes/#pengertian","text":"Naive Bayes adalah salah satu algoritma klasifikasi yang paling sederhana dan paling cepat. Algoritma ini berdasarkan pada teorema Bayes dengan asumsi independensi antar variabel. Algoritma ini sangat cepat karena hanya melakukan perhitungan pada data yang ada saja, tidak memerlukan proses training. Algoritma ini sangat cocok untuk digunakan pada data yang memiliki banyak atribut, karena algoritma ini tidak memerlukan proses training. Keunggulan NB adalah efektif dan cepat sehingga NB bisa digunakan pada aplikasi spam filtering dan deteksi anomali di jaringan komputer.","title":"Pengertian"},{"location":"docs/naive-bayes/#cara-kerja-nb","text":"NB adalah sekumpulan algoritma klasifikasi yuang dibangun berdasarkan Teori Bayes (Thomas Bayes, seorang ahli matematika dari Inggris di abad ke-18). Prinsipnya adalah menghitung seberapa tinggi kemubgkinan satu example dalam suatu observasi untuk masuk ke dalam suatu kelas, dengan memanfaatkan training dataset untuk menghitung kemungkinan setiap kelas berdasarkan nilai-nilai feature di dalamnya. Dalam Statistik teori ini dipakai untuk menjelaskan conditional probability , yaitu kemungkinan munculnya suatu kejadian A bila suatu kejadian B muncul, karena kejadian A bergantung pada kejadian B maka disebut conditional atau bersyarat. Sebagai contoh membuat model prediksi SMS termasuk pesan sampah atau bukan. Dalam kasus ini, feature yang digunakan adalah kata-kata yang ada di dalam SMS. Dengan menggunakan teori Bayes, kita bisa menghitung kemungkinan SMS termasuk pesan sampah atau bukan dengan menghitung kemungkinan setiap kata-kata yang ada di dalam SMS termasuk pesan sampah atau bukan. Pesan sampah kebanyakan berisi kata \"kredit\" katakanlah jumlahnya 10% dari seluruh pesan SMS, lalu ditulis sebagai P(\"kredit\") = 0,1. Lalu dari pengamatan sebelumnya diperoleh misalnya 30% dari seluruh pesan adalah pesan sampah, sehinggga ditulis P(sampah) = 0,3. Apabila keduanya digabungkan maka conditional propability dapat dihitung nilainya, yang menunjukkan kemungkinan SMS dianggap sampah jika mengandung kata \"kredit\". Secara matematis dapat ditulis sebagai berikut: \\(P(sampah|\"kredit\") = \\frac{P(\"kredit\"|sampah)P(sampah)}{P(\"kredit\")}\\) Apabila posterior propability melampaui suatu ambang batas , maka SMS dianggap sampah. Misalnya ambang batasnya adalah 0,5, maka SMS dianggap sampah apabila \\(P(sampah|\"kredit\") > 0,5\\) .","title":"Cara Kerja NB"},{"location":"docs/naive-bayes/#membuat-model-nb","text":"Langkah pertama dalam membuat model NB adalah mengubah data yang ada menjadi suatu tabel frekuensi Berisi Kata Kredit Pesan SMS Ada Tidak Ada Jumlah Sampah 8 22 30 Bukan Sampah 2 68 30 Jumlah 10 90 100 Selanjutnya membuat tabel kemungkinan berdasarkan tabel frekuensi untuk mengukur likehood atau kemungkinan kemunculan kata \"Kredit\" Berisi Kata Kredit Pesan SMS Ada Tidak Ada Jumlah Kemungkinan Sampah 8/30=0,27 22/30=0,73 30 0,3 Bukan Sampah 2/70=0,03 68/70=0,97 70 0.7 Kemungkinan 10/100=0,1 90/100=0,9 P(\"Kredit\"|Sampah) = 0,27 P(Sampah) = 0,3 Kemungkinan kata \"kredit\" muncul di sluruh pesan SMS yaitu P(\"kredit\") = 0,1 Dengan menggunakan posterior probability maka dapat dihitung nilai kemungkinan SMS dianggap sampah jika mengandung kata \"kredit\" yaitu P(Sampah|\"kredit\") = 0,27 * 0,3 / 0,1 = 0,81 Nilainya posterior probability yang tinggi sehingga bisa menentukan pesan apapun yang berisi kata \"kredit\" adalah pasti pesan sampah.","title":"Membuat Model NB"},{"location":"docs/naive-bayes/#rumus-conditional-probability","text":"Secara matematis: \\(P(A|B) = \\frac{P(B|A)P(A)}{P(B)} = \\frac{P(B \\cap A)}{P(B)}\\) Dengan: P(A) : kemungkinan A terjadi P(A|B) : kemungkinan A terjadi jika B terjadi P(B) : kemungkinan B terjadi P(B|A) : kemungkinan B terjadi jika A terjadi P(B \u2229 A) : kemungkinan B dan A terjadi Scikit-learn menyediakan pilihan untuk membuat model NB dengan menggunakan metode Gaussian, Multinomial dan Bernoulli. Metode Gaussian digunakan untuk data yang berupa angka, sedangkan metode Multinomial digunakan untuk data yang berupa kata-kata dan metode Bernoulli digunakan untuk data yang berupa kata-kata yang bernilai 0 atau 1.","title":"Rumus Conditional Probability"},{"location":"docs/naive-bayes/#kelebihan-dan-kekurangan","text":"Kelebihan NB adalah mudah untuk diimplementasikan dan memiliki performa yang baik. Namun, NB memiliki kekurangan yaitu tidak dapat mengklasifikasikan data yang tidak terdapat dalam data latih. Selain itu, NB juga tidak dapat mengklasifikasikan data yang memiliki nilai yang sama.","title":"kelebihan dan kekurangan"},{"location":"docs/neural-network/","text":"Neural network \u00b6 Multi Layer Perceptron \u00b6 Multi layer perception adalah salah satu jenis neural network yang paling umum digunakan. Multi layer perception terdiri dari 3 layer, yaitu input layer, hidden layer, dan output layer. Input layer merupakan layer pertama yang menerima input dari data. Hidden layer merupakan layer yang berada di tengah-tengah antara input layer dan output layer. Output layer merupakan layer terakhir yang menghasilkan output dari data. Yang perlu diperhatikan adalah setiap node dalam dsatu lapisan akan tersambung secara penuh ke semua node di lapisan berikutnya. Informasi yang mengalir dalam satu arah yaitu dari kiri ke kanan yang disebut dengan jaringan feedforward . Hubungan antara node juga akan diberikan angka bobot masing-masing yang menentukan seberapa penting feature yang bersangkutan. Jika ada 10 feature maka akan ada 10 input node. Multi Layer Perceptron: flowchart LR id1[(Data)] --> id2[Input Layer] id1 --> id3[Hidden Layer] id1 --> id4[Output Layer] subgraph Input Layer id2((X1)) id3((X2)) id4((X3)) end id2 --> id5 id2 --> id6 id3 --> id5 id3 --> id6 id4 --> id5 id4 --> id6 subgraph Hidden Layer id5((H1)) id6((H2)) end id5 --> id7 id6 --> id7 subgraph Output Layer id7((Y)) end id7 --> output Jumlah Hiden Layer bergantung pada seberapa kopmleks pembelajaran yang mau ditentukan. Semakin kompleks pembelajaran maka semakin banyak hidden layer yang dibutuhkan. Jumlah hidden layer juga bergantung pada jumlah node yang ada di dalamnya. Semakin banyak node maka semakin kompleks pembelajaran yang bisa dilakukan. Jumlah node juga bergantung pada jumlah feature yang ada. Semakin banyak feature maka semakin banyak node yang dibutuhkan. Jumlah node juga bergantung pada jumlah output yang diinginkan. Semakin banyak output yang diinginkan maka semakin banyak node yang dibutuhkan. Deep Learning menggunakan Multi Layer Perceptron dengan jumlah hidden layer yang sangat banyak. Jumlah hidden layer yang banyak akan membuat pembelajaran menjadi lebih kompleks. Semakin kompleks pembelajaran maka semakin baik hasil prediksi yang dihasilkan. Cara Membangun Model MLP \u00b6 MLP adalah metode supervised learning oleh sebab itu membutuhkan label di training dataset . Langkah selanjutnyta adalah melakukan training dengan mekanisme interatif yang disebut dengan backpropagation , yang bisa dianalogikan proses bejalar dari kesalahan. Proses backpropagation akan menghitung error dari setiap node dan mengubah bobot dari setiap node agar error semakin kecil. Proses backpropagation akan berhenti jika error sudah sangat kecil atau sudah mencapai batas maksimal iterasi yang ditentukan. Contoh Implementasi MLP \u00b6 Penggunaan MLP dengan menggunakan library scikit-learn. Berikut adalah contoh implementasi MLP dengan menggunakan library scikit-learn. from sklearn.neural_network import MLPClassifier from sklearn.model_selection import train_test_split Metode ini bisa digunakan untuk klasifikasi dan regresi. Untuk klasifikasi, MLPClassifier menggunakan fungsi aktivasi logistic untuk output layer. Untuk regresi, MLPRegressor menggunakan fungsi aktivasi identity untuk output layer. Untuk klasifikasi, MLPClassifier menggunakan fungsi aktivasi logistic untuk output layer. Untuk regresi, MLPRegressor menggunakan fungsi aktivasi identity untuk output layer. Import semua modul yang kita perlukan seperti Pandaas, Neural Network dan model_selection import pandas as pd import sklearn.neural_network as ann import sklearn.model_selection as ms df_occupancy = pd . read_csv ( 'datatraining.csv' , header = 0 , names = [ \"id\" , \"date\" , \"Temperature\" , \"Humadity\" , \"Light\" , \"CO2\" , \"HumadityRatio\" , \"Occupancy\" ]) Kemudian buang id dan date karena tidak diperlukan untuk proses training X = df_occupancy . drop ([ 'id' , 'date' ], axis = 1 ) y = df_occupancy [ 'Occupancy' ] kemudian dilakukan profiling singkat dari kelima feature tersebut X . describe () Ada 8.143 baris data, perbedaan antara nilai maksimum dan minimum cukup besar, misalnya suhu dari 19 hingga 23, cahaya memiliki jangkauan dari nol hingga 1.546 sehinga perbedaannya cukup besar, sedangkan MLP sangat sensitif terhadap dengan perbedaan seperti ini (lain halnya dengan algoritma Decision Tree atau Naive Bayes). Untuk itu, kita perlu melakukan normalisasi terhadap data-data tersebut. Split Data \u00b6 Sebelum proses training dilakukan, kita perlu membagi data menjadi dua bagian, yaitu data training dan data testing. Data training digunakan untuk proses training, sedangkan data testing digunakan untuk menguji akurasi dari model yang kita buat. Untuk membagi data, kita bisa menggunakan fungsi train_test_split dari modul model_selection yang sudah kita import sebelumnya. X_train , X_test , y_train , y_test = ms . train_test_split ( X , y , test_size = 0.2 ) X_train . count () Normalisasi Data \u00b6 Selanjutnya, kita perlu melakukan normalisasi terhadap data-data tersebut. Scikit menyediakan sklearn.preprocessing.MinMaxScaler untuk melakukan normalisasi. Training dataset sekarang jangkauan angkanya hanya dari sekitar negatif 2 hingga positif 7 print ( X_train . min ()) print ( X_train . max ()) -1.6244629234944807 7.290207701659988 Training Model \u00b6 MLPClassifier memerukan dua parameter penting, yaitu hidden layer dan jumlah maksimal iterasi. Untuk parameter hidden layer , kita bisa menentukan sendiri berapa banyak hidden layer yang kita inginkan., jadi diawali dengan angka acat yaitu tida node di hidden layer dan lima kali iterasi maksimal: mlp = ann . MLPClassifier ( hidden_layer_sizes = ( 3 ), max_iter = 5 ) mlp . fit ( X_train , y_train ) Model sudah terbentuk dan bisa diuji dengan data testing. Untuk menguji akurasi dari model, kita bisa menggunakan fungsi score dari model yang sudah kita buat. Hasil prediksi dari model yang sudah kita buat bisa kita lihat dengan menggunakan fungsi predict dari model yang sudah kita buat. y_prediksi = mlp . predict ( X_test ) Dengan menggunakan sklearn.metrics, kita bisa menghitung akurasi dari model yang sudah kita buat. import sklearn.metrics as met print ( met . classification_report ( y_test , y_prediksi )) Dari hasil di atas angka precision class 0 bernilai 0,83 dan clas 1 bernilai 1. Angka precision bisa berbeda-beda ketika dijalankan. Hal ini dikarenakan proses training menggunakan fungsi random . Jika kita ingin mendapatkan hasil yang sama, kita bisa menambahkan parameter random_state pada fungsi train_test_split. Hasil yang lebih tinggi bisa didapatkan dengan menambahkan hidden layer dan iterasi maksimal. Namun, semakin banyak hidden layer dan iterasi maksimal, maka proses training akan semakin lama. Untuk itu, kita perlu mencari kombinasi yang tepat antara jumlah hidden layer dan iterasi maksimal. Untuk mengurangi model overvit , kita harus memastikan training dataset kita cukup besar dan membatasi jumlah hidden layer . Ada beberapa pilihan activation function yang bisa kita gunakan, yaitu identity , logistic , tanh , dan relu . Contohnya: from sklearn.neural_network import MLPClassifier mlp = ann . MLPClassifier ( hidden_layer_sizes = ( 3 ), max_iter = 5 , activation = 'logistic' ) Setelah model terbentuk, kita bisa mengecek bobot dari setiap hidden layer dan output layer dengan menggunakan fungsi coef . print ( mlp . coefs_ ) Kekurangan model MLP adalah sulit untuk membaca hasilnya, angka-angka bobot di atas tidak bisa langsung menunjukan feature mana yang lebih penting dibandingkan dengan feature lainnya.","title":"Neural Network"},{"location":"docs/neural-network/#neural-network","text":"","title":"Neural network"},{"location":"docs/neural-network/#multi-layer-perceptron","text":"Multi layer perception adalah salah satu jenis neural network yang paling umum digunakan. Multi layer perception terdiri dari 3 layer, yaitu input layer, hidden layer, dan output layer. Input layer merupakan layer pertama yang menerima input dari data. Hidden layer merupakan layer yang berada di tengah-tengah antara input layer dan output layer. Output layer merupakan layer terakhir yang menghasilkan output dari data. Yang perlu diperhatikan adalah setiap node dalam dsatu lapisan akan tersambung secara penuh ke semua node di lapisan berikutnya. Informasi yang mengalir dalam satu arah yaitu dari kiri ke kanan yang disebut dengan jaringan feedforward . Hubungan antara node juga akan diberikan angka bobot masing-masing yang menentukan seberapa penting feature yang bersangkutan. Jika ada 10 feature maka akan ada 10 input node. Multi Layer Perceptron: flowchart LR id1[(Data)] --> id2[Input Layer] id1 --> id3[Hidden Layer] id1 --> id4[Output Layer] subgraph Input Layer id2((X1)) id3((X2)) id4((X3)) end id2 --> id5 id2 --> id6 id3 --> id5 id3 --> id6 id4 --> id5 id4 --> id6 subgraph Hidden Layer id5((H1)) id6((H2)) end id5 --> id7 id6 --> id7 subgraph Output Layer id7((Y)) end id7 --> output Jumlah Hiden Layer bergantung pada seberapa kopmleks pembelajaran yang mau ditentukan. Semakin kompleks pembelajaran maka semakin banyak hidden layer yang dibutuhkan. Jumlah hidden layer juga bergantung pada jumlah node yang ada di dalamnya. Semakin banyak node maka semakin kompleks pembelajaran yang bisa dilakukan. Jumlah node juga bergantung pada jumlah feature yang ada. Semakin banyak feature maka semakin banyak node yang dibutuhkan. Jumlah node juga bergantung pada jumlah output yang diinginkan. Semakin banyak output yang diinginkan maka semakin banyak node yang dibutuhkan. Deep Learning menggunakan Multi Layer Perceptron dengan jumlah hidden layer yang sangat banyak. Jumlah hidden layer yang banyak akan membuat pembelajaran menjadi lebih kompleks. Semakin kompleks pembelajaran maka semakin baik hasil prediksi yang dihasilkan.","title":"Multi Layer Perceptron"},{"location":"docs/neural-network/#cara-membangun-model-mlp","text":"MLP adalah metode supervised learning oleh sebab itu membutuhkan label di training dataset . Langkah selanjutnyta adalah melakukan training dengan mekanisme interatif yang disebut dengan backpropagation , yang bisa dianalogikan proses bejalar dari kesalahan. Proses backpropagation akan menghitung error dari setiap node dan mengubah bobot dari setiap node agar error semakin kecil. Proses backpropagation akan berhenti jika error sudah sangat kecil atau sudah mencapai batas maksimal iterasi yang ditentukan.","title":"Cara Membangun Model MLP"},{"location":"docs/neural-network/#contoh-implementasi-mlp","text":"Penggunaan MLP dengan menggunakan library scikit-learn. Berikut adalah contoh implementasi MLP dengan menggunakan library scikit-learn. from sklearn.neural_network import MLPClassifier from sklearn.model_selection import train_test_split Metode ini bisa digunakan untuk klasifikasi dan regresi. Untuk klasifikasi, MLPClassifier menggunakan fungsi aktivasi logistic untuk output layer. Untuk regresi, MLPRegressor menggunakan fungsi aktivasi identity untuk output layer. Untuk klasifikasi, MLPClassifier menggunakan fungsi aktivasi logistic untuk output layer. Untuk regresi, MLPRegressor menggunakan fungsi aktivasi identity untuk output layer. Import semua modul yang kita perlukan seperti Pandaas, Neural Network dan model_selection import pandas as pd import sklearn.neural_network as ann import sklearn.model_selection as ms df_occupancy = pd . read_csv ( 'datatraining.csv' , header = 0 , names = [ \"id\" , \"date\" , \"Temperature\" , \"Humadity\" , \"Light\" , \"CO2\" , \"HumadityRatio\" , \"Occupancy\" ]) Kemudian buang id dan date karena tidak diperlukan untuk proses training X = df_occupancy . drop ([ 'id' , 'date' ], axis = 1 ) y = df_occupancy [ 'Occupancy' ] kemudian dilakukan profiling singkat dari kelima feature tersebut X . describe () Ada 8.143 baris data, perbedaan antara nilai maksimum dan minimum cukup besar, misalnya suhu dari 19 hingga 23, cahaya memiliki jangkauan dari nol hingga 1.546 sehinga perbedaannya cukup besar, sedangkan MLP sangat sensitif terhadap dengan perbedaan seperti ini (lain halnya dengan algoritma Decision Tree atau Naive Bayes). Untuk itu, kita perlu melakukan normalisasi terhadap data-data tersebut.","title":"Contoh Implementasi MLP"},{"location":"docs/neural-network/#split-data","text":"Sebelum proses training dilakukan, kita perlu membagi data menjadi dua bagian, yaitu data training dan data testing. Data training digunakan untuk proses training, sedangkan data testing digunakan untuk menguji akurasi dari model yang kita buat. Untuk membagi data, kita bisa menggunakan fungsi train_test_split dari modul model_selection yang sudah kita import sebelumnya. X_train , X_test , y_train , y_test = ms . train_test_split ( X , y , test_size = 0.2 ) X_train . count ()","title":"Split Data"},{"location":"docs/neural-network/#normalisasi-data","text":"Selanjutnya, kita perlu melakukan normalisasi terhadap data-data tersebut. Scikit menyediakan sklearn.preprocessing.MinMaxScaler untuk melakukan normalisasi. Training dataset sekarang jangkauan angkanya hanya dari sekitar negatif 2 hingga positif 7 print ( X_train . min ()) print ( X_train . max ()) -1.6244629234944807 7.290207701659988","title":"Normalisasi Data"},{"location":"docs/neural-network/#training-model","text":"MLPClassifier memerukan dua parameter penting, yaitu hidden layer dan jumlah maksimal iterasi. Untuk parameter hidden layer , kita bisa menentukan sendiri berapa banyak hidden layer yang kita inginkan., jadi diawali dengan angka acat yaitu tida node di hidden layer dan lima kali iterasi maksimal: mlp = ann . MLPClassifier ( hidden_layer_sizes = ( 3 ), max_iter = 5 ) mlp . fit ( X_train , y_train ) Model sudah terbentuk dan bisa diuji dengan data testing. Untuk menguji akurasi dari model, kita bisa menggunakan fungsi score dari model yang sudah kita buat. Hasil prediksi dari model yang sudah kita buat bisa kita lihat dengan menggunakan fungsi predict dari model yang sudah kita buat. y_prediksi = mlp . predict ( X_test ) Dengan menggunakan sklearn.metrics, kita bisa menghitung akurasi dari model yang sudah kita buat. import sklearn.metrics as met print ( met . classification_report ( y_test , y_prediksi )) Dari hasil di atas angka precision class 0 bernilai 0,83 dan clas 1 bernilai 1. Angka precision bisa berbeda-beda ketika dijalankan. Hal ini dikarenakan proses training menggunakan fungsi random . Jika kita ingin mendapatkan hasil yang sama, kita bisa menambahkan parameter random_state pada fungsi train_test_split. Hasil yang lebih tinggi bisa didapatkan dengan menambahkan hidden layer dan iterasi maksimal. Namun, semakin banyak hidden layer dan iterasi maksimal, maka proses training akan semakin lama. Untuk itu, kita perlu mencari kombinasi yang tepat antara jumlah hidden layer dan iterasi maksimal. Untuk mengurangi model overvit , kita harus memastikan training dataset kita cukup besar dan membatasi jumlah hidden layer . Ada beberapa pilihan activation function yang bisa kita gunakan, yaitu identity , logistic , tanh , dan relu . Contohnya: from sklearn.neural_network import MLPClassifier mlp = ann . MLPClassifier ( hidden_layer_sizes = ( 3 ), max_iter = 5 , activation = 'logistic' ) Setelah model terbentuk, kita bisa mengecek bobot dari setiap hidden layer dan output layer dengan menggunakan fungsi coef . print ( mlp . coefs_ ) Kekurangan model MLP adalah sulit untuk membaca hasilnya, angka-angka bobot di atas tidak bisa langsung menunjukan feature mana yang lebih penting dibandingkan dengan feature lainnya.","title":"Training Model"},{"location":"docs/pandas/","text":"Dasar-Dasar Pandas \u00b6 Langkah pertama yang harus dilakukan yaitu mengimpor library pandas: import pandas as pd Untuk mempermudah, kita akan menggunakan pd sebagai alias dari pandas. Konspep pandas yaitu menyimpan data yang disebut Series dan DataFrame . Series adalah kolom tunggal, sedangkan DataFrame adalah tabel yang terdiri dari beberapa kolom atau series. Series: \u00b6 mangga 0 1 1 2 2 3 mangga = pd . Series ([ 1 , 2 , 3 ]) Pandas akan otomatis membuat index untuk series, jika tidak ingin menggunakan index default, kita bisa menambahkan parameter index: mangga = pd . Series ([ 1 , 2 , 3 ], index = [ 'a' , 'b' , 'c' ]) Index tidak harus berupa angka numerik, bisa kita ganti dengan string. Dataframe \u00b6 DataFrame tidak jauh beda dengan array NumPy yaitu berupa tabel dua dimensi dengan baris dan kolom. Cara membuat DataFrame adalah dengan menggunakan dictionary: df1 = pd . DataFrame ({ 3 , 0 , 9 }), columns = [ 'apel' ], index = [ 1 , 2 , 3 ]) apel 1 3 2 0 3 9 Untuk DataFrame dengan dua kolom, kita bisa menggunakan dictionary dengan dua key: dict1 = { 'apel' : [ 3 , 0 , 9 ], 'jeruk' : [ 1 , 4 , 2 ]} df1 = pd . DataFrame ( dict1 ) apel jeruk 0 3 1 1 0 4 2 9 2 Setelah DataFrame terbentuk kita bebas memanipulasinya. Menghitung jumlah data dengan fungsi count() dan juga menghitung jumlah keseluruhan semua nilai sum() df1 . count () Output apel 3 jeruk 3 dtype: int64 df1 . sum () Output apel 12 jeruk 7 dtype: int64 Menghitung rata-rata dengan fungsi mean() df1 . mean () Output apel 4.000000 jeruk 2.333333 dtype: float64 Memapilkan statistik singkat tentang dataset seperti jumlah data, angka rata-rata, angka minimal, angka maksimum, standart devation , dan sebagainya df1 . describe () Output apple jeruk count 3.000000 3.000000 mean 4.000000 2.333333 std 4.582576 1.527525 min 0.000000 1.000000 25% 1.500000 1.500000 50% 3.000000 2.000000 75% 6.000000 3.000000 max 9.000000 4.000000 Mengambil Subset dari DataFrame \u00b6 Untuk mengambil subset dari Dataframe dalam Python dapat menggunakan operator indeks dan slicing. Contoh mengguanakn slicing (potong data) untuk mengambil baris 0 hingga 2. df . loc [ start : end ] # untuk baris, berdasarkan label df . iloc [ start : end ] # untuk baris, berdasarkan posisi indeks df1 . loc [ 0 : 2 ] output apel jeruk 0 3 1 1 0 4 2 9 2","title":"Pandas"},{"location":"docs/pandas/#dasar-dasar-pandas","text":"Langkah pertama yang harus dilakukan yaitu mengimpor library pandas: import pandas as pd Untuk mempermudah, kita akan menggunakan pd sebagai alias dari pandas. Konspep pandas yaitu menyimpan data yang disebut Series dan DataFrame . Series adalah kolom tunggal, sedangkan DataFrame adalah tabel yang terdiri dari beberapa kolom atau series.","title":"Dasar-Dasar Pandas"},{"location":"docs/pandas/#series","text":"mangga 0 1 1 2 2 3 mangga = pd . Series ([ 1 , 2 , 3 ]) Pandas akan otomatis membuat index untuk series, jika tidak ingin menggunakan index default, kita bisa menambahkan parameter index: mangga = pd . Series ([ 1 , 2 , 3 ], index = [ 'a' , 'b' , 'c' ]) Index tidak harus berupa angka numerik, bisa kita ganti dengan string.","title":"Series:"},{"location":"docs/pandas/#dataframe","text":"DataFrame tidak jauh beda dengan array NumPy yaitu berupa tabel dua dimensi dengan baris dan kolom. Cara membuat DataFrame adalah dengan menggunakan dictionary: df1 = pd . DataFrame ({ 3 , 0 , 9 }), columns = [ 'apel' ], index = [ 1 , 2 , 3 ]) apel 1 3 2 0 3 9 Untuk DataFrame dengan dua kolom, kita bisa menggunakan dictionary dengan dua key: dict1 = { 'apel' : [ 3 , 0 , 9 ], 'jeruk' : [ 1 , 4 , 2 ]} df1 = pd . DataFrame ( dict1 ) apel jeruk 0 3 1 1 0 4 2 9 2 Setelah DataFrame terbentuk kita bebas memanipulasinya. Menghitung jumlah data dengan fungsi count() dan juga menghitung jumlah keseluruhan semua nilai sum() df1 . count () Output apel 3 jeruk 3 dtype: int64 df1 . sum () Output apel 12 jeruk 7 dtype: int64 Menghitung rata-rata dengan fungsi mean() df1 . mean () Output apel 4.000000 jeruk 2.333333 dtype: float64 Memapilkan statistik singkat tentang dataset seperti jumlah data, angka rata-rata, angka minimal, angka maksimum, standart devation , dan sebagainya df1 . describe () Output apple jeruk count 3.000000 3.000000 mean 4.000000 2.333333 std 4.582576 1.527525 min 0.000000 1.000000 25% 1.500000 1.500000 50% 3.000000 2.000000 75% 6.000000 3.000000 max 9.000000 4.000000","title":"Dataframe"},{"location":"docs/pandas/#mengambil-subset-dari-dataframe","text":"Untuk mengambil subset dari Dataframe dalam Python dapat menggunakan operator indeks dan slicing. Contoh mengguanakn slicing (potong data) untuk mengambil baris 0 hingga 2. df . loc [ start : end ] # untuk baris, berdasarkan label df . iloc [ start : end ] # untuk baris, berdasarkan posisi indeks df1 . loc [ 0 : 2 ] output apel jeruk 0 3 1 1 0 4 2 9 2","title":"Mengambil Subset dari DataFrame"}]}